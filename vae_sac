



class AgentCustomObservation(AgentWithConverter):

   
    def init_deep_q(self, transformed_observation):

        self.deep_q = vae_SAC(state_dim=transformed_observation.shape[-1], hidden_dim1=self.hidden_dim1,
                                        hidden_dim2=self.hidden_dim2, z_hidden_dim=self.z_hidden_dim, z_dim=self.z_dim,
                                        action_dim=self.action_space.size(), actor_lr=self.actor_lr,
                                        critic_lr=self.critic_lr, alpha_lr=self.alpha_lr,
                                        target_entropy=self.target_entropy,
                                        tau=self.tau, gamma=self.gamma, device=self.device,
                                        cost_weight=self.cost_weight, writer=self.writer)
        # self.vae = VAE(transformed_observation.shape[-1], z_hidden_dim, z_dim).to(device)  # TODO 确定下神经网络的结构

    def __init__(self, env,
                 action_space,
                 replay_buffer,
                 target_entropy,
                 actor_lr,
                 critic_lr,
                 alpha_lr,
                 tau,
                 gamma,
                 device,
                 cost_weight,
                 hidden_dim1,
                 hidden_dim2,
                 z_hidden_dim,
                 z_dim,
                 batch_size,
                 minimal_size,
                 logdir):
        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct)
        '''
        print("转换后动作空间大小为{}".format(self.action_space.size()))
        # self.my_neural_network = AwesomeNN()
        # self.my_neural_networl.load(path)
        for i in range(self.action_space.size()):
            print("\n第{}个动作为:".format(i))
            print(self.convert_act(i))
            '''
        self.env = env
        self.deep_q = None
        self.replay_buffer = replay_buffer
        self.target_entropy = target_entropy
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr
        self.alpha_lr = alpha_lr
        self.tau = tau
        self.gamma = gamma
        self.device = device
        self.cost_weight = cost_weight
        self.hidden_dim1 = hidden_dim1
        self.hidden_dim2 = hidden_dim2
        self.z_hidden_dim = z_hidden_dim
        self.z_dim = z_dim
        self.batch_size = batch_size
        self.minimal_size = minimal_size
        self.logdir = logdir
        self.writer = SummaryWriter(logdir)
        # self.csv_file = open('training_results_redisp1_200K.csv', 'w', newline='')
        # self.csv_writer = csv.writer(self.csv_file)
        # self.csv_writer.writerow(
        # ['total_step', 'mean_reward_30', 'mean_alive_30', 'mean_reward_50', 'mean_alive_50', 'mean_reward_100',
        # 'mean_alive_100'])


    def train(self):
        env = self.env
        epoch_rewards = []  # 记录每轮的总奖励
        epoch_alive = []  # 记录每轮的存活步数
        total_step = 0
        for epoch in range(1500):
            obs = env.reset()
            state = self.convert_obs(obs)
            if self.deep_q is None:
                self.init_deep_q(state)

            total_reward = 0  # 当前轮的总奖励
            alive_steps = 0  # 当前轮的步数
            done = False  # 是否完成当前 episode

            while not done and alive_steps < 1000:  # 每轮限制在 2000 步以内

                # 选择动作并与环境交互
                total_step += 1
                state1 = state
                state2 = torch.tensor([state1], dtype=torch.float).to(self.device)
                mu, logvar = self.deep_q.vae.encoder(state2)
                z = self.deep_q.vae.reparameterize(mu, logvar).detach()
                action = self.deep_q.take_action(z)
                act = self.convert_act(action)
                next_obs, reward, done, info = env.step(act)
                # 转换下一状态并存储到 replay buffer
                next_state = self.convert_obs(next_obs)
                self.replay_buffer.add(state, action, reward, next_state, done)
                # 更新模型
                if self.replay_buffer.size() > self.minimal_size:
                    b_s, b_a, b_r, b_ns, b_d = self.replay_buffer.sample(self.batch_size)
                    transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r,
                                       'dones': b_d}
                    self.deep_q.update(transition_dict)
                    self.deep_q.update_vae(transition_dict)

                # 更新当前状态、奖励和步数
                state = next_state
                total_reward += reward
                alive_steps += 1

            print(
                f"Episode finished: Survived [{alive_steps}] steps, Total reward [{total_reward}]，evalcost:[{evalcost}]")  # 记录每轮的奖励和存活步数
            epoch_rewards.append(total_reward)
            epoch_alive.append(alive_steps)

        return epoch_rewards, epoch_alive




class VAEEncoder(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, z_dim):
        super(VAEEncoder, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2_mu = torch.nn.Linear(hidden_dim, z_dim)
        self.fc2_logvar = torch.nn.Linear(hidden_dim, z_dim)

    def forward(self, x):
        h1 = F.relu(self.fc1(x))
        mu = self.fc2_mu(h1)
        logvar = self.fc2_logvar(h1)

        # 限制logvar的范围，避免过大值
        logvar = torch.clamp(logvar, min=-5, max=5)  # 适当调整范围以防止溢出
        return mu, logvar


class VAEDecoder(torch.nn.Module):
    def __init__(self, z_dim, hidden_dim, state_dim):
        super(VAEDecoder, self).__init__()
        self.fc1 = torch.nn.Linear(z_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, state_dim)

    def forward(self, z):
        h1 = F.relu(self.fc1(z))
        return torch.sigmoid(self.fc2(h1))  # Sigmoid for reconstruction, output values between 0 and 1


class VAE(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, z_dim):
        super(VAE, self).__init__()
        self.encoder = VAEEncoder(state_dim, hidden_dim, z_dim)
        self.decoder = VAEDecoder(z_dim, hidden_dim, state_dim)

    def forward(self, x):
        # 数据归一化
        x_normalized = self.normalize_input(x)

        # 通过编码器获取均值和对数方差
        mu, logvar = self.encoder(x_normalized)

        # 通过重参数化技巧得到潜在变量z
        z = self.reparameterize(mu, logvar)

        # 通过解码器重构输入
        reconstructed_x = self.decoder(z)

        return reconstructed_x, mu, logvar

    def normalize_input(self, x):
        """对输入进行标准化，假设输入数据为特征张量（batch_size, state_dim）"""
        mean = x.mean(dim=0, keepdim=True)
        std = x.std(dim=0, keepdim=True)
        return (x - mean) / (std + 1e-8)  # 加上小的epsilon防止除0错误

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def loss_function(self, reconstructed_x, x, mu, logvar):
        # 计算重构误差 (MSE)
        MSE = F.mse_loss(reconstructed_x, x, reduction='sum')

        # 计算KL散度
        KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

        # 输出 MSE 和 KL 损失
        print(f"MSE Loss: {MSE.item()}")
        print(f"KL Loss: {KL.item()}")

        return MSE + KL


class vae_SAC:
    ''' 处理离散动作的带约束SAC算法 '''

    def __init__(self, state_dim, hidden_dim1, hidden_dim2, z_hidden_dim, z_dim, action_dim, actor_lr, critic_lr,
                 alpha_lr, target_entropy, tau, gamma, device, cost_weight, writer):
        self.update_step = 0
        self.vae = VAE(state_dim, z_hidden_dim, z_dim).to(device)
        self.vae_optimizer = torch.optim.Adam(self.vae.parameters(),
                                              lr=1e-5)

        self.writer = writer

        # 策略网络
        self.actor = PolicyNet(z_dim, hidden_dim1, hidden_dim2, action_dim).to(device)
        # 第一个Q网络
        self.critic_1 = QValueNet(z_dim, hidden_dim1, hidden_dim2, action_dim).to(device)
        # 第二个Q网络
        self.critic_2 = QValueNet(z_dim, hidden_dim1, hidden_dim2, action_dim).to(device)
        self.target_critic_1 = QValueNet(z_dim, hidden_dim1, hidden_dim2,
                                         action_dim).to(device)  # 第一个目标Q网络
        self.target_critic_2 = QValueNet(z_dim, hidden_dim1, hidden_dim2,
                                         action_dim).to(device)  # 第二个目标Q网络
        # 令目标Q网络的初始参数和Q网络一样
        self.target_critic_1.load_state_dict(self.critic_1.state_dict())
        self.target_critic_2.load_state_dict(self.critic_2.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),
                                                   lr=critic_lr)
        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),
                                                   lr=critic_lr)

        self.gamma = gamma
        self.tau = tau
        self.device = device



    def take_action(self, z):  # 有一个疑问

        probs = self.actor(z)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        return action.item()

    # 计算目标Q值,直接用策略网络的输出概率进行期望计算
    def calc_target(self, rewards,  next_states, dones):
        mu_next, logvar_next = self.vae.encoder(next_states)
        z_next = self.vae.reparameterize(mu_next, logvar_next).detach()
        next_probs = self.actor(z_next)
        next_log_probs = torch.log(next_probs + 1e-8)
        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)
        q1_value = self.target_critic_1(z_next)
        q2_value = self.target_critic_2(z_next)
        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),
                               dim=1,
                               keepdim=True)
        next_value = min_qvalue + self.log_alpha.exp() * entropy
        # print("rewards shape",rewards.shape)
        # print("next_value_shape",next_value.shape)

        td_target = rewards + self.gamma * next_value * (1 - dones)

        return td_target


    def update_vae(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)  # 动作不再是float类型
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
       
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        print("states shape:", states.shape)

        reconstructed_x, mu, logvar = self.vae(states)  # 使用VAE编码器对当前状态进行编码
        print("reconstrucred_x", reconstructed_x)
        vae_loss = self.vae.loss_function(reconstructed_x, states, mu, logvar)  # 计算VAE的损失
        self.vae_optimizer.zero_grad()
        vae_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.vae.parameters(), max_norm=1.0)
        self.vae_optimizer.step()
        self.writer.add_scalar('VAE', vae_loss.item(), self.update_step)
        print(f"VAE loss: {vae_loss.item()}")

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(
            self.device)  # 动作不再是float类型
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
     
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        # print("states shape:", states.shape)
        # print("actions shape:", actions.shape)
        # print("rewards shape:", rewards.shape)
        # print("next_states shape:", next_states.shape)
        # print("dones shape:", dones.shape)

        mu, logvar = self.vae.encoder(states)
        # 获取潜在表示 z
        z = self.vae.reparameterize(mu, logvar).detach()  # 从VAE的均值和对数方差中采样潜在表示 z
        # 更新两个Q网络
        td_target = self.calc_target(rewards,  next_states, dones)  # TODO
        critic_1_q_values = self.critic_1(z).gather(1, actions)
        critic_1_loss = torch.mean(
            F.mse_loss(critic_1_q_values, td_target.detach()))
        critic_2_q_values = self.critic_2(z).gather(1, actions)
        critic_2_loss = torch.mean(
            F.mse_loss(critic_2_q_values, td_target.detach()))
        self.critic_1_optimizer.zero_grad()
        critic_1_loss.backward()
        self.critic_1_optimizer.step()
        self.critic_2_optimizer.zero_grad()
        critic_2_loss.backward()
        self.critic_2_optimizer.step()

        # 更新策略网络
        probs = self.actor(z)
        log_probs = torch.log(probs + 1e-8)
        # 直接根据概率计算熵
        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #
        q1_value = self.critic_1(z)
        q2_value = self.critic_2(z)
        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),
                               dim=1,
                               keepdim=True)  # 直接根据概率计算期望
        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)
     
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
       

        # 更新alpha值
        alpha_loss = torch.mean(
            (entropy - self.target_entropy).detach() * self.log_alpha.exp())
        self.log_alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.log_alpha_optimizer.step()



        self.writer.add_scalar('Loss/Critic1', critic_1_loss.item(), self.update_step)
        self.writer.add_scalar('Loss/Critic2', critic_2_loss.item(), self.update_step)
        self.writer.add_scalar('Loss/Actor', actor_loss.item(), self.update_step)
        self.writer.add_scalar('Loss/Alpha', alpha_loss.item(), self.update_step)

        # self.writer.add_scalar('Loss/lam', lam_loss.item(), self.update_step)
        self.writer.add_scalar('Alpha', self.log_alpha.exp().item(), self.update_step)
        # self.writer.add_scalar('lam', self.lam, self.update_step)
        self.update_step += 1

        self.soft_update(self.critic_1, self.target_critic_1)
        self.soft_update(self.critic_2, self.target_critic_2)



